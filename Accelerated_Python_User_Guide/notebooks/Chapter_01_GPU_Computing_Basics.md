第 1 章：GPU 计算基础
引言
图形处理器 (GPU) 是一种专门的电子电路，最初设计用于加速计算机图形和图像处理。GPU 拥有专门的处理核心，可用于加速计算过程。

虽然 GPU 最初是为处理图像和视觉数据而设计的，但现在它们正被用于增强其他计算过程，如科学计算和机器学习。这是因为 GPU 可以有效地并行用于大规模分布式计算过程。

GPU 编程领域非常广阔，理解 GPU 编程概念和 Python GPU 编程范式可能会让人望而生畏。在本指南中，我们希望揭开这个生态系统的神秘面纱，引导开发人员找到适合其特定加速计算需求的正确解决方案。



并行模式
GPU 的主要优势在于并行性，即同时处理一个整体的各个部分。并行处理实现中使用了四种架构，包括：

单指令单数据 (SISD)
单指令多数据 (SIMD)
多指令单数据 (MISD)
多指令多数据 (MIMD)
大多数 CPU 是多核处理器，以 MIMD 架构运行。相比之下，GPU 使用 SIMD 架构。这种差异使得 GPU 非常适合需要对大量数据项执行相同过程的高度并行过程。

通用 GPU 编程
与 GPU 的原始用途相关，这些处理器以前要求用户了解专门的语言，如 OpenGL。这些语言仅用于 GPU，学习起来不切实际，并造成了使用障碍。

2007 年，随着 NVIDIA CUDA 框架的推出，这一障碍被打破，提供了更广泛的 GPU 资源访问。CUDA 基于 C 语言，并提供了一个 API，开发人员可以利用该 API 将 GPU 处理应用于其他任务。

CPU vs. GPU 的优缺点
在构建程序以在 CPU 或 GPU 上运行时，请记住这两者具有不同的架构。指令不能在两个单元之间共享，它们也不会自动将工作从一个转移到另一个。程序员必须通过使用支持 GPU 的软件包，有意地显式或隐式地将数据和处理从一个转移到另一个。

CPUs
中央处理器 (CPU) 是当今计算机中使用最广泛的处理器类型，长期以来一直是数据分析任务的主力。CPU 由多个核心组成，可以顺序或并行执行指令，使其适用于广泛的应用。

CPU 的优势
CPU 有几个优势：

通用性：CPU 处理各种通用任务，如逻辑比较、寄存器间的数据传输和文件管理。在处理 RAM 中的数据处理、I/O 操作和操作系统管理等操作时，CPU 比 GPU 更快。
多任务处理：CPU 可以同时运行多个任务。
高时钟频率：CPU 的时钟频率通常高于 GPU，这使它们能够更快地执行单线程任务。
灵活性：CPU 可以在多项活动之间进行多任务切换，并快速切换上下文。
缓存：CPU 具有较大的本地缓存。
兼容性：CPU 兼容所有类型的系统，而 GPU 可能需要专门的硬件。
CPU 的弱点
尽管有优势，CPU 也有一些局限性：

有限的并行处理：CPU 并非为高度并行的工作负载设计。
高功耗：CPU 会消耗大量电力，从而增加运营成本。
内存带宽限制：与 GPU 相比，CPU 的内存带宽有限，这在处理大型数据集时可能导致性能下降。
并行处理：CPU 不太擅长需要数百万次相同操作的任务。
发展有限：CPU 是一项非常成熟的技术，改进潜力较小，而 GPU 改进潜力较大。
总的来说，CPU 是许多任务的通用且可靠的选择，但对于计算密集型、高度并行的工作负载或时间与预算有限的大规模操作，可能不是最佳选择。

GPUs
图形处理器 (GPU) 最初设计用于渲染图形和图像，但近年来已成为其他通用计算任务的强大加速器。GPU 由数千个处理核心组成，可以并行执行指令，使其成为计算密集型任务的理想选择。

GPU 的优势
GPU 有几个优势：

并行处理：GPU 可以同时对许多数据点进行并行计算。GPU 拥有数百个核心，允许进行大规模并行计算，如矩阵乘法。
高内存带宽：GPU 可以更快地访问和操作数据。
能源效率：GPU 的功耗可能低于 CPU，从而降低运营成本。
可定制架构：GPU 可以针对特定应用和工作负载进行定制，具有灵活性和适应性。
科学计算用例：GPU 为生成式 AI、深度学习和大数据处理等专门任务提供了巨大的加速。
GPU 的弱点
尽管有优势，GPU 也有一些局限性，包括：

多任务处理限制：GPU 可以大规模重复执行一项任务，但不能像 CPU 那样执行通用计算任务。
复杂性：GPU 在处理分支逻辑、顺序操作或其他复杂的编程模式时表现不佳。
软件兼容性：GPU 可能需要额外的编程或优化才能有效工作。
总的来说，GPU 是需要高性能和并行处理的任务的强大且高效的选择。这使得它们成为加速科学 Python 代码的理想选择，因为这些代码往往会对大量数据执行重复操作。

GPU 架构
为了了解如何充分利用 GPU，我们需要了解 GPU 架构的基础知识。为了简单起见，我们可以将单个 GPU 设备视为由多个流式多处理器 (Streaming Multiprocessor) 集群组成。

GPU 架构简化图



单个 GPU 设备由多个处理器集群 (PC) 组成，这些集群包含多个流式多处理器 (SM)。每个 SM 都包含一层用于其关联核心的 1 级 (L1) 指令缓存。通常，一个 SM 在从全局 GDDR-5（或较新 GPU 型号中的 GDDR-6）显存拉取数据之前，使用专用的 1 级缓存和共享的 2 级 (L2) 缓存。其架构对内存延迟具有耐受性。

与 CPU 相比，GPU 的显存缓存层较少且相对较小，这是因为更多的晶体管被用于计算。GPU 不太关心从内存中检索数据需要多长时间，只要通过大规模并行计算能够隐藏内存访问的“延迟”即可。

GPU 针对数据并行吞吐量计算进行了优化。通过观察核心数量，可以快速了解其具备的并行处理能力。

这就是 GPU 编程的力量。如果能够利用大量核心并优化数据传输，就能大幅加速应用程序。

NVIDIA GPU 的演进
有许多选项可供选择，包括消费级 GPU、数据中心 GPU 和托管工作站。



消费级 GPU
消费级 GPU 不适用于大规模深度学习项目，但可以作为实现的切入点。这些 GPU 使能够廉价地补充现有系统，并可用于模型构建或低级测试。

有关 NVIDIA GeForce 显卡的最新信息，请访问：https://www.nvidia.com/en-us/geforce/graphics-cards/

数据中心 GPU
数据中心 GPU 是生产级深度学习实现的标准。这些 GPU 专为大规模项目设计，可以提供企业级性能。

有关 NVIDIA 数据中心解决方案的最新信息，请访问：https://www.nvidia.com/en-us/data-center/data-center-gpus/

DGX 服务器
NVIDIA DGX 服务器是企业级全栈解决方案。这些系统专门为机器学习和深度学习操作而设计。系统即插即用，可以部署在裸金属服务器或容器中。

有关 NVIDIA DGX 服务器的最新信息，请访问：https://www.nvidia.com/en-us/data-center/dgx-platform/

常见的 GPU 加速应用
GPU 通常用于以下任务：

科学模拟和建模
机器学习和神经网络
深度学习
科学计算
GPU 可以与 CPU 协同工作，快速执行数值密集型操作，如科学模拟和建模。GPU 设计传统上侧重于最大化浮点单元和执行多维数组操作。它们特别适合线性数学或矩阵操作的应用。这得益于 GPU 的 TensorCore 单元，可用于优化的矩阵乘法。

许多科学计算问题都有 GPU 实现，包括分子建模、流体动力学、天体物理学和气候模拟。

深度学习
神经网络从海量数据中学习，试图模拟人脑的行为。在训练阶段，神经网络扫描输入数据并将其与标准数据进行比较，以便形成预测和预报。

由于神经网络主要处理海量数据集，训练时间会随着数据集的增长而增加。虽然可以使用 CPU 训练较小规模的神经网络，但 CPU 处理这些海量数据的效率较低，导致随着层数和参数的增加，训练时间也会增加。

神经网络构成了深度学习的基础（具有三层或更多层的神经网络），并且设计为并行运行，每个任务独立于其他任务。这使得 GPU 更适合处理用于训练神经网络的海量数据集和复杂的数学数据。

大规模并行输入的深度学习
在 NVIDIA 推出 CUDA 之后，开发了多个深度学习框架，如 PyTorch 和 TensorFlow。这些框架抽象化了直接使用 CUDA 编程的复杂性，使 GPU 处理在现代深度学习实现中变得触手可及。

深度学习模型是具有三层或更多层的神经网络。深度学习模型具有高度灵活的架构，可以直接从原始数据中学习。使用大型数据集训练深度学习网络可以提高其预测准确性。

在深度学习中，CPU 的效率低于 GPU，因为它们按顺序一次处理一个任务。随着越来越多的数据点用于输入和预报，CPU 管理所有相关任务变得更加困难。

深度学习需要极高的速度和高性能，并且当所有操作同时进行时，模型学习得更快。由于拥有数千个核心，GPU 针对训练深度学习模型进行了优化，处理多个并行任务的速度比 CPU 快达三倍。

资源
NVIDIA GPU 架构：https://www.nvidia.com/en-us/technologies/

支持 NVIDIA CUDA 的处理器：https://developer.nvidia.com/cuda-gpus

NVIDIA 加速应用目录：https://www.nvidia.com/en-us/accelerated-applications/

NVIDIA GPU 云应用目录：https://catalog.ngc.nvidia.com/