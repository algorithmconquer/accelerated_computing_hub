CUDA Core 教程 - 底层 GPU 编程
目录
cuda.core 简介
环境设置
理解 CUDA 概念
内存管理
核函数编译与执行
错误处理
练习
1. cuda.core 简介
cuda.core 模块提供了对 CUDA 驱动 API 的直接访问，使您能够最大限度地控制 GPU 编程。与高级 API 不同，cuda.core 要求您手动管理所有内容：

上下文管理：创建和管理执行上下文
内存分配：显式分配和释放 GPU 内存
核函数编译：在运行时编译 CUDA C/C++ 代码
同步：管理流（streams）和事件（events）
何时使用 cuda.core：
非常适用于：

以对 Python 友好的方式学习 GPU 编程
快速原型化 GPU 算法
现有库未提供的自定义并行算法
当您需要对 GPU 资源进行精细控制时
2. 环境设置
先决条件
具有 CUDA 能力的 NVIDIA GPU
CUDA 驱动版本 12.2 或更高
Python 3.8+
安装
(此处跳过代码)

此操作的作用：

cuda-core：安装 CUDA Core Python 包
numpy：安装用于数组操作的 NumPy
验证
以下 Python 代码片段检查 CUDA 是否已正确设置：

(此处跳过代码)

这是运行任何 GPU 工作负载之前的良好第一步。如果此步骤失败，您的驱动程序或安装可能不正确。

3. 理解 CUDA 概念
关键术语
主机 vs 设备：

主机 (Host)：您的 CPU 和系统内存
设备 (Device)：您的 GPU 和显存
执行模型：

核函数 (Kernel)：在 GPU 上运行的函数
线程 (Thread)：单个执行单元
线程块 (Block)：可以协作的一组线程
网格 (Grid)：线程块的集合
内存层次结构：

全局内存 (Global Memory)：主 GPU 内存（速度慢但容量大）
共享内存 (Shared Memory)：线程块内共享的快速内存
寄存器 (Registers)：速度最快的内存，每个线程私有
可以将寄存器想象成您的办公桌（访问速度快，空间有限），共享内存想象成您团队的档案柜（团队成员访问速度快，空间较大），全局内存想象成公司的仓库（空间巨大，但取东西需要时间）。

设备管理与上下文
什么是设备和上下文？ 在 CUDA 中，设备 (Device) 代表您的 GPU 硬件。上下文 (Context) 就像该 GPU 上的一个工作区，您的程序可以在其中运行。可以将设备想象为物理 GPU 卡，将上下文想象为该卡上的个人工作空间。

尝试从基础的设备管理开始：

(此处跳过代码)

此操作的作用：

Device(0)：创建一个代表第一个 GPU 的设备对象（GPU 编号从 0 开始）
device.set_current()：告诉 CUDA“我想使用这个 GPU 进行操作”
如果您有多个 GPU，CUDA 需要知道您想使用哪一个，这就是我们需要 set_current 的原因。

获取设备属性
让我们进一步了解我们的 GPU：

(此处跳过代码)

这些属性的含义：

计算能力 (Compute capability)：类似于 GPU 的“版本号”，较高的数字支持更多特性
多处理器数量 (Multiprocessor count)：您的 GPU 拥有多少个“处理器组”（越多 = 并行能力越强）
每个线程块的最大线程数 (Max threads per block)：您可以放入一个线程块的最大线程数量
最大线程块维度 (Max block dimensions)：您在线程块中排列线程的方式（1D、2D 或 3D）
设备同步
有时您需要等待 GPU 完成其所有工作：

(此处跳过代码)

什么时候需要同步？

在将结果从 GPU 复制回 CPU 之前
在测量 GPU 操作耗时之前
在关闭程序之前
这几乎就像在说：“在所有工人完成当前任务之前，不要继续执行。”

4. 内存管理
理解 GPU 内存
GPU 内存与计算机的主内存 (RAM) 是分开的。要使用 GPU，您需要：

在 GPU 内存中分配空间
将数据从 CPU 复制到 GPU
在 GPU 上处理数据
将结果从 GPU 复制回 CPU
CPU 和 GPU 拥有独立的内存系统，并针对各自不同的任务进行了优化。

(此处跳过代码)

此操作的作用：

计算大小：我们计算出需要多少字节（1000 个浮点数 × 每个 4 字节）
分配内存：device.allocate() 在 GPU 上预留空间
获取缓冲区：返回的 device_buffer 就像指向我们 GPU 内存的“句柄”
重要提示：就像常规 Python 编程一样，分配内存并不会在其中放入任何有意义的数据。它只是预留的空白空间。

5. 核函数编译与执行
第一个核函数：向量加法
(此处跳过代码)

解析：

extern "C"：告诉编译器使用 C 风格的函数名称
__global__：将其标记为核函数（在 GPU 上运行）
float *a, float *b, float *c：指向 GPU 内存中数组的指针
tid = threadIdx.x + blockIdx.x * blockDim.x：计算唯一的线程 ID
threadIdx.x：此线程在其线程块内的位置
blockIdx.x：此线程属于哪个线程块
blockDim.x：每个线程块中有多少个线程
c[i] = a[i] + b[i]：每个线程执行的实际计算
为什么要进行复杂的索引计算？ 假设您有 1000 个元素需要处理，每块有 256 个线程：

线程块 0：线程 0-255 处理元素 0-255
线程块 1：线程 0-255 处理元素 256-511
线程块 2：线程 0-255 处理元素 512-767
线程块 3：线程 0-255 处理元素 768-999
每个线程都需要知道它应该处理哪个元素。

(此处跳过代码)

启动配置深入探讨：

线程块大小 (Block size)：每个线程块 256 个线程（常见选择，2 的幂效果较好）
网格大小 (Grid size)：
(N + block_size - 1) // block_size
 确保我们有足够的线程
对于 N=1000 和 block_size=256：grid_size = (1000 + 255) // 256 = 4 个线程块
总线程数 = 4 × 256 = 1024 个线程（多于我们的 1000 个元素，这没问题）
为什么要使用 grid_size 计算公式？

此公式确保我们始终拥有足够的线程：

如果 N=1000 且 block_size=256，我们至少需要 4 个线程块
如果 N=256 且 block_size=256，我们恰好需要 1 个线程块
如果 N=257 且 block_size=256，我们需要 2 个线程块
高级核函数示例
我们现在可以尝试将两个矩阵相乘：

(此处跳过代码)

核函数解释：

2D 线程布局：每个线程处理结果矩阵的一个元素
row = blockIdx.y * blockDim.y + threadIdx.y：此线程计算哪一行
col = blockIdx.x * blockDim.x + threadIdx.x：此线程计算哪一列
点积计算：对于元素 C[row][col]，我们计算：
所有 k 的 A[row][k] × B[k][col] 之和
这是矩阵乘法的数学定义
2D 启动配置：
线程块排列在 2D 网格中，以匹配矩阵的 2D 特性
每个线程块为 16×16 个线程（每个线程块共 256 个线程）
为什么要使用 2D 布局？

矩阵乘法自然地映射到 2D：每个线程计算一个输出元素，输出元素排列在 2D 网格中（即结果矩阵）。

6. 错误处理
在底层 CUDA 驱动 API 级别工作时，您负责在每次 API 调用后检查错误。

GPU 编程可能以多种方式失败：

内存不足：请求的 GPU 内存超过可用内存
无效核函数：CUDA C 代码中的漏洞
设备错误：硬件问题或驱动程序问题
启动失败：无效的网格/线程块配置
良好的错误处理可以帮助您：

快速调试问题
编写健壮的应用程序
提供有用的错误消息
(此处跳过代码)

错误处理最佳实践：

使用 try-except 块：将 CUDA 操作包装在 try-except 中
特定异常：尽可能捕获特定的错误类型
有用的消息：解释出了什么问题以及如何修复
清理：使用 finally 块编写清理代码
不要忽略错误：始终处理或传播异常
常见错误模式
(此处跳过代码)

展示的内容：

设备错误：错误的设备编号，缺少 GPU
内存错误：申请过多内存
编译错误：CUDA C 代码中的语法错误
启动错误：无效的线程/线程块配置
7. 练习：向量操作
编写一个执行两个向量逐元素相乘的 CUDA 核函数。

(此处跳过代码)

(此处跳过代码)

资源
CUDA Python 参考：https://numba.pydata.org/numba-doc/dev/cuda-reference/

仓库：https://github.com/NVIDIA/cuda-python

